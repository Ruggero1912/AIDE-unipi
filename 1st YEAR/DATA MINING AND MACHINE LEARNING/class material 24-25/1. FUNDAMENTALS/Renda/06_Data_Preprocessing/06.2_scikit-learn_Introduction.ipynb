{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [`scikit-learn` intro](#scikit-learn-intro)\n",
    "    - [Data representation](#Data-Representation)\n",
    "    - [The `Estimator` interface](#The-scikit-learn's-estimator-interface)\n",
    "    - [The `sklearn.datasets` module](#The-sklearn.datasets-module)\n",
    "- [Data preprocessing in scikit-learn](#Data-preprocessing-in-scikit-learn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `scikit-learn` intro\n",
    "[`scikit-learn`](http://scikit-learn.org), or `sklearn`, is a Python package designed to give access to well-known **machine learning algorithms within Python** code, through a clean, well-thought-out API. It has been built by hundreds of contributors from around the world, and is used across industry and academia.\n",
    "\n",
    "`scikit-learn` is built upon Python's [`NumPy`](http://www.numpy.org/) (Numerical Python) and [`SciPy`](http://www.scipy.org/) (Scientific Python) libraries, which enable efficient in-core numerical and scientific computation within Python. As such, scikit-learn is not specifically designed for extremely large datasets, though there is some work in this area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation\n",
    "\n",
    "Most machine learning algorithms implemented in scikit-learn expect a **two-dimensional array or matrix** `X`, usually represented as a NumPy ndarray. The expected shape of `X` is `(n_samples, n_features)`.\n",
    "\n",
    "* `n_samples`: The number of samples, where each sample is an item to process (e.g., classify). A sample can be a document, a picture, a sound, a video, a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits.\n",
    "* `n_features`:\tThe number of features or distinct traits that can be used to describe each item in a quantitative manner. \n",
    "\n",
    "The *supervised* machine learning algorithms implemented in scikit-learn also expect a **one-dimensional array** `y` with shape `(n_samples,)`. This array associated a target class to every sample in the input `X`.\n",
    "\n",
    "![data-layout.png](images/data-layout.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning\n",
    "\n",
    "![supervised machine learning overview](images/plot_ML_flow_chart_12.png)\n",
    "\n",
    "In the supervised learning paradigm the training data (observations, measurements, etc.) are accompanied by labels indicating the class of the\n",
    "observations.\n",
    "\n",
    "After training, the fitted model does no longer expect the label `y` as an input: it will try to predict the most likely labels `y_new` for a new set of samples `X_new`.\n",
    "\n",
    "Depending on the nature of the target `y`, supervised learning can be given different names:\n",
    "\n",
    "* If `y` has values in a fixed set of **categorical outcomes** (represented by integers) the task to predict `y` is called **classification**.\n",
    "* If `y` has **floating point values** (e.g. to represent a price, a temperature, a size...), the task to predict `y` is called **regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Learning\n",
    "Unsupervised learning addresses a different sort of problem. Here the class labels of training data is unknown, and we are interested in finding similarities between the observations. \n",
    "An unsupervised learning algorithm only uses a single set of observations `X` with shape `(n_samples, n_features)` and does not use any kind of labels.\n",
    "\n",
    "Unsupervised learning comprises tasks such as *dimensionality reduction* and *clustering*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `scikit-learn`'s estimator interface\n",
    "\n",
    "Scikit-learn strives to have a uniform interface across all methods. \n",
    "The main objects in scikit-learn are the following: \n",
    "- estimator (base object)\n",
    "- predictor\n",
    "- transformer\n",
    "\n",
    "Note that one class can implement multiple interfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimator\n",
    "Every algorithm is exposed in scikit-learn via an **estimator** object\n",
    "\n",
    "The behaviour of an estimator typically depends on a number of parameters. The parameters of an estimator can be set when it is instantiated or by modifying the corresponding attribute:\n",
    "- `estimator = Estimator(param1=1, param2=2)`\n",
    "  \n",
    "An Estimator implements a `fit` method to learn from data, either:\n",
    "- `estimator = estimator.fit(data, targets)`, for supervised learning applications.\n",
    "- `estimator = estimator.fit(data)`, for unsupervised learning applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictor\n",
    "For supervised learning, or some unsupervised problems, the **predictor** object implements the following method:\n",
    "\n",
    "- `prediction = predictor.predict(data)`: given a \"fitted\" model, predict the label of a new set of data. This method accepts one argument, the array of observations, and returns the predicted label for each observation in the array.\n",
    "\n",
    "Classification algorithms usually also offer a way to quantify certainty of a prediction, either using `decision_function` or `predict_proba`:\n",
    "\n",
    "- `probability = predictor.predict_proba(data)`: clearly, the label with the highest probability is returned by `predictor.predict`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer\n",
    "The transformer object is used for filtering or modifying the data, in a supervised or unsupervised way. It implements the following method:\n",
    "- `new_data = transformer.transform(data)`\n",
    "\n",
    "When fitting and transforming can be performed much more efficiently together than separately, implements:\n",
    "\n",
    "- ` new_data = transformer.fit_transform(data)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the [docs](https://scikit-learn.org/stable/index.html)**\n",
    "\n",
    "![landscape](images/sklearn_landscape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `sklearn.datasets` module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.datasets` [docs](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous lectures, we have imported the iris-dataset from a \".csv\" file\n",
    "```python\n",
    "iris_df = pd.read_csv(os.path.join('dataset', 'iris.csv'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, scikit-learn comes with a few standard datasets. The `sklearn.datasets` module includes utilities to load datasets, including methods to load and fetch popular reference datasets. It also features some artificial data generators.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Iris dataset\n",
    "The library embeds a copy of the Iris CSV file along with a helper function to load it into NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:20:17.025725Z",
     "start_time": "2020-06-29T13:20:16.933133Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features of each sample flower are stored in the `data` attribute of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:20:17.044284Z",
     "start_time": "2020-06-29T13:20:17.029214Z"
    }
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = iris.data.shape\n",
    "\n",
    "print(n_samples)\n",
    "print(n_features)\n",
    "print(iris.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information about the class of each sample is stored in the `target` attribute of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.data.shape, iris.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:20:17.057979Z",
     "start_time": "2020-06-29T13:20:17.049112Z"
    }
   },
   "outputs": [],
   "source": [
    "print(iris.target)\n",
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Know the class distribution from numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(iris.target, return_counts = True)\n",
    "unique, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, converting to a Pandas Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(iris.target).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate artificial data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X1, Y1 = make_blobs(n_samples = [1000, 100, 500],\n",
    "                    n_features = 2, \n",
    "                    random_state = 1)\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.title(\"Synthetic normally distributed dataset\")\n",
    "plt.scatter(X1[:, 0],\n",
    "            X1[:, 1], \n",
    "            marker = \"o\", \n",
    "            c = Y1, \n",
    "            s = 25, \n",
    "            edgecolor = \"k\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `random_state` to get reproducible results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axes = plt.subplots(1,3,figsize=(15, 5))\n",
    "X1, Y1 = make_blobs(n_samples = [1000, 100, 500], n_features = 2, random_state = 42)\n",
    "X2, Y2 = make_blobs(n_samples = [1000, 100, 500], n_features = 2, random_state = 42)\n",
    "X3, Y3 = make_blobs(n_samples = [1000, 100, 500], n_features = 2, random_state = 21)\n",
    "\n",
    "axes[0].scatter(X1[:, 0], X1[:, 1], marker = \"o\", c = Y1, s = 25, edgecolor = \"k\")\n",
    "axes[0].set_title('random_state = 42')\n",
    "\n",
    "axes[1].scatter(X2[:, 0], X2[:, 1], marker = \"o\", c = Y2, s = 25, edgecolor = \"k\")\n",
    "axes[1].set_title('random_state = 42')\n",
    "\n",
    "axes[2].scatter(X3[:, 0], X3[:, 1], marker = \"o\", c = Y3, s = 25, edgecolor = \"k\")\n",
    "axes[2].set_title('random_state = 21')\n",
    "\n",
    "f.suptitle('The effect of random seed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seed is used for initializing the pseudorandom data generator:\n",
    "- the two processes with *seed=42* lead to exactly the same result\n",
    "- the process with *seed=21* leads to a different result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing in scikit-learn\n",
    "\n",
    "    - ☑️ in previous notebooks\n",
    "    - ✅ in this notebook\n",
    "\n",
    "Major Tasks in data preprocessing:\n",
    "- Data cleaning\n",
    "    - Handling missing values (on HYPOTHYROID dataset) ☑️ (e.g., `sklearn.impute`, `pd.DataFrame.fillna`)\n",
    "    - Smooth noisy data\n",
    "    - Identify or remove outliers\n",
    "    - Resolve inconsistencies\n",
    "    - <font color='blue'>[Encoding categorical features](#Encoding-categorical-features)</font> ✅(e.g.,  `sklearn.preprocessing.OneHotEncoder`, `OrdinalEncoder`)    \n",
    "- Data integration\n",
    "- Data reduction\n",
    "    - <font color='blue'>[Dimensionality reduction](#Dimensionality-reduction:-PCA)</font>  ✅ (e.g., `sklearn.decomposition.PCA`)\n",
    "    - Numerosity reduction\n",
    "        - Parametric methods\n",
    "        - Non parametric methods\n",
    "            - Sampling\n",
    "                - Simple random sampling with replacement\n",
    "                - Simple random sampling without replacement\n",
    "- Data transformation and data discretization\n",
    "    - <font color=blue>[Normalization](#Data-Normalization)</font> ✅ (e.g.,  `sklearn.preprocessing.MinMaxScaler`, `StandaradScaler`, `RobustScaler`)\n",
    "    - Concept Hierarchy Generation\n",
    "    - Discretization with pandas (on IRIS dataset) ☑️ (e.g., `pd.cut` and `pd.qcut`) \n",
    "        - Equal width binning ☑️\n",
    "        - Equal frequency binning ☑️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical features\n",
    "\n",
    "Most sklearn estimators expect numerical variables in input. \n",
    "It is crucial to encode categorical variables into a numeric representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal encoder\n",
    "Convert ordinal categorical features to integer codes. (Notice that an assumption is implicitly made on the magnitude of successive values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [['Low', 'Low'], ['Medium', 'High'], ['High', 'Low']]\n",
    "pd.DataFrame(X, columns = ['X','Y']) # pretty print as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder()  # by default, lexicographic order\n",
    "enc.fit(X)\n",
    "X_encoded = enc.transform(X)\n",
    "\n",
    "pd.DataFrame(X_encoded, columns = ['X','Y']) # pretty print as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OrdinalEncoder(categories = [['Low', 'Medium', 'High'], ['Low', 'Medium', 'High']]) # set custom order\n",
    "enc.fit(X)\n",
    "X_encoded = enc.transform(X)\n",
    "\n",
    "pd.DataFrame(X_encoded, columns = ['X','Y']) # pretty print as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHot encoder\n",
    "\n",
    "Encode nominal categorical features as a one-hot numeric array. (Nominal = 'categories, states, or \"names of things\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder()\n",
    "\n",
    "     # mainland,  browser,   smoker\n",
    "X = [['US',      'Safari',  'yes'], \n",
    "     ['Europe',  'Firefox', 'no'], \n",
    "     ['Europe',  'Chrome',  'yes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X, columns = ['mainland', 'browser', 'smoker']) # pretty print as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.fit(X)\n",
    "X_encoded = enc.transform(X).toarray()\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.get_feature_names_out(['mainland', 'browswer', 'smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_encoded, columns = enc.get_feature_names_out(['mainland', 'browswer', 'smoker'])) # pretty print as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(drop = 'first') # 'first' drop the first category in each feature\n",
    "\n",
    "     # mainland,  browser,   smoker\n",
    "X = [['US',      'Safari',  'yes'], \n",
    "     ['Europe',  'Firefox', 'no'], \n",
    "     ['Europe',  'Chrome',  'yes']]\n",
    "\n",
    "enc.fit(X)\n",
    "X_encoded = enc.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc.get_feature_names_out(['mainland','browswer','smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_encoded, columns = enc.get_feature_names_out(['mainland', 'browswer', 'smoker'])) # pretty print as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the same transformation can be obtained by using the pandas `pd.get_dummies` utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [['US', 'Safari', 'yes'], \n",
    "     ['Europe', 'Firefox', 'no'], \n",
    "     ['Europe', 'Chrome', 'yes']]\n",
    "df = pd.DataFrame(X,columns = ['mainland','browser','smoker'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction: PCA\n",
    "\n",
    "Principle Component Analysis (PCA) finds a projection that captures the largest amount of variation in data. It can be effectively used as a dimensionality reduction technique. \n",
    "\n",
    "\n",
    "Consider the Iris dataset. It cannot be visualized in a single 2D plot, as it has 4 features. We are going to extract 2 combinations of sepal and petal dimensions to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T13:20:18.366686Z",
     "start_time": "2020-06-29T13:20:18.170838Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, scaling (actually, standardizing = rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one) is important, for how PCA is performed in sklearn.\n",
    "\n",
    "PCA is a variance maximization exercize: \n",
    "- Suppose you describe a group of people in terms of height (in meters) and weight (in kilos).\n",
    "- Height varies less than weight because of their respective scales.\n",
    "- PCA might determine that the direction of maximal variance more closely corresponds with the 'weight' axis, if those features are not scaled. This is misleading, as a change in height of one meter can be considered much more important than the change in weight of one kilogram.\n",
    "\n",
    "\n",
    "In this case, however, we can directly apply PCA to the original dataset as the range of variables is comparable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.95)\n",
    "pca.fit(X)\n",
    "X_reduced = pca.transform(X)\n",
    "print(\"Reduced dataset shape:\", X_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_reduced[:, 0], \n",
    "            X_reduced[:, 1], \n",
    "            c = y, \n",
    "            cmap = 'Paired')\n",
    "plt.xlabel('Component #1')\n",
    "plt.ylabel('Component #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "df = px.data.iris()\n",
    "pca = PCA(n_components = 3)\n",
    "components = pca.fit_transform(X) # notice that we are coupling fit and transform in a single statement\n",
    "\n",
    "total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    components, x = 0, y = 1, z = 2, color = y,\n",
    "    title = f'Total Explained Variance: {total_var:.2f}%',\n",
    "    labels = {'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'},\n",
    "    height = 600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "px.area(\n",
    "    x = range(1, exp_var_cumul.shape[0] + 1),\n",
    "    y = exp_var_cumul,\n",
    "    labels = {\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Iris dataset, the first principal component explains more than 90% of the variance of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(pd.DataFrame(X), alpha = 1, figsize = (10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 4)\n",
    "transformed_X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_matrix(pd.DataFrame(transformed_X), \n",
    "               alpha = 1,\n",
    "               figsize = (10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plots changes after PCA transformation, and suggest how PCA finds uncorrelated components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "Extracted from [here](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some plotting utilities from above source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib import colors, colorbar\n",
    "\n",
    "# plasma does not exist in matplotlib < 1.5\n",
    "cmap = getattr(cm, \"plasma_r\", cm.hot_r)\n",
    "\n",
    "\n",
    "def create_axes(title, figsize=(16, 6)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    # define the axis for the first plot\n",
    "    left, width = 0.1, 0.22\n",
    "    bottom, height = 0.1, 0.7\n",
    "    bottom_h = height + 0.15\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the zoomed-in plot\n",
    "    left = width + left + 0.2\n",
    "    left_h = left + width + 0.02\n",
    "\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom_h, width, 0.1]\n",
    "    rect_histy = [left_h, bottom, 0.05, height]\n",
    "\n",
    "    ax_scatter_zoom = plt.axes(rect_scatter)\n",
    "    ax_histx_zoom = plt.axes(rect_histx)\n",
    "    ax_histy_zoom = plt.axes(rect_histy)\n",
    "\n",
    "    # define the axis for the colorbar\n",
    "    left, width = width + left + 0.13, 0.01\n",
    "\n",
    "    rect_colorbar = [left, bottom, width, height]\n",
    "    ax_colorbar = plt.axes(rect_colorbar)\n",
    "\n",
    "    return (\n",
    "        (ax_scatter, ax_histy, ax_histx),\n",
    "        (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n",
    "        ax_colorbar,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_distribution(axes, X, y, hist_nbins=50, title=\"\", x0_label=\"\", x1_label=\"\"):\n",
    "    ax, hist_X1, hist_X0 = axes\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x0_label)\n",
    "    ax.set_ylabel(x1_label)\n",
    "\n",
    "    # The scatter plot\n",
    "    colors = cmap(y)\n",
    "    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker=\"o\", s=5, lw=0, c=colors)\n",
    "\n",
    "    # Removing the top and the right spine for aesthetics\n",
    "    # make nice axis layout\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "    ax.spines[\"left\"].set_position((\"outward\", 10))\n",
    "    ax.spines[\"bottom\"].set_position((\"outward\", 10))\n",
    "\n",
    "    # Histogram for axis X1 (feature 5)\n",
    "    hist_X1.set_ylim(ax.get_ylim())\n",
    "    hist_X1.hist(\n",
    "        X[:, 1], bins=hist_nbins, orientation=\"horizontal\", color=\"grey\", ec=\"grey\"\n",
    "    )\n",
    "    hist_X1.axis(\"off\")\n",
    "\n",
    "    # Histogram for axis X0 (feature 0)\n",
    "    hist_X0.set_xlim(ax.get_xlim())\n",
    "    hist_X0.hist(\n",
    "        X[:, 0], bins=hist_nbins, orientation=\"vertical\", color=\"grey\", ec=\"grey\"\n",
    "    )\n",
    "    hist_X0.axis(\"off\")\n",
    "    \n",
    "    \n",
    "def make_plot(title, data):\n",
    "    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n",
    "    axarr = (ax_zoom_out, ax_zoom_in)\n",
    "    # full data\n",
    "    plot_distribution(\n",
    "        axarr[0],\n",
    "        data,\n",
    "        y,\n",
    "        hist_nbins=200,\n",
    "        x0_label=feature_mapping[features[0]],\n",
    "        x1_label=feature_mapping[features[1]],\n",
    "        title=\"Full data\",\n",
    "    )\n",
    "\n",
    "    # zoom-in\n",
    "    zoom_in_percentile_range = (0, 99)\n",
    "    cutoffs_X0 = np.percentile(data[:, 0], zoom_in_percentile_range)\n",
    "    cutoffs_X1 = np.percentile(data[:, 1], zoom_in_percentile_range)\n",
    "\n",
    "    non_outliers_mask = np.all(data > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) & np.all(\n",
    "        data < [cutoffs_X0[1], cutoffs_X1[1]], axis=1\n",
    "    )\n",
    "    plot_distribution(\n",
    "        axarr[1],\n",
    "        data[non_outliers_mask],\n",
    "        y[non_outliers_mask],\n",
    "        hist_nbins=50,\n",
    "        x0_label=feature_mapping[features[0]],\n",
    "        x1_label=feature_mapping[features[1]],\n",
    "        title=\"Zoom-in\",\n",
    "    )\n",
    "\n",
    "    # colorbar\n",
    "    norm = colors.Normalize(y_full.min(), y_full.max())\n",
    "    colorbar.ColorbarBase(\n",
    "        ax_colorbar,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        orientation=\"vertical\",\n",
    "        label=\"Color mapping for values of y\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the california housing datasets using the sklearn utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "dataset = fetch_california_housing()\n",
    "feature_names = dataset.feature_names\n",
    "feature_mapping = {\n",
    "    \"MedInc\": \"Median income in block group\", # <------- !\n",
    "    \"HousAge\": \"Median house age in block group\",\n",
    "    \"AveRooms\": \"Average number of rooms per household\",\n",
    "    \"AveBedrms\": \"Average number of bedrooms per household\",\n",
    "    \"Population\": \"Block group population\",\n",
    "    \"AveOccup\": \"Average house occupancy (number of household members)\", # <------- !\n",
    "    \"Latitude\": \"Block group latitude\",\n",
    "    \"Longitude\": \"Block group longitude\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "X_full, y_full = dataset.data, dataset.target\n",
    "y = minmax_scale(y_full) # scale the output between 0 and 1 for the colorbar\n",
    "X_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A *block group* is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "A *household* is a group of people residing within a home. Since the average number of rooms and bedrooms in this dataset are provided per household, these columns may take surpinsingly large values for block groups with few households and many empty houses, such as vacation resorts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable is the **median house value** for California districts, expressed in hundreds of thousands of dollars ($100,000).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.concatenate((dataset.data,\n",
    "                                  np.expand_dims(dataset.target, -1)),\n",
    "                                 axis = 1),\n",
    "                  columns = dataset.feature_names + ['MedianHouseVal'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['MedInc','AveOccup']].plot(kind = 'box', \n",
    "                               subplots = True, \n",
    "                               figsize = (8, 5),\n",
    "                               layout = (1, 2),\n",
    "                               sharey = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the maximum value of \"Average Occupancy\" (AveOccup) is far beyond the value of the third quartile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"MedInc\", \"AveOccup\"]\n",
    "features_idx = [feature_names.index(feature) for feature in features]\n",
    "X = X_full[:, features_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze the behaviour of the following **scalers**:\n",
    "- StandardScaler\n",
    "- MinMaxScaler\n",
    "- RobustScaler\n",
    "\n",
    "Notice that all of them operate on the various features independently.\n",
    "\n",
    "**Plot structure**:\n",
    "- left plot: entire dataset\n",
    "- right plot: zoomed-in plot to show the dataset without the marginal outliers (keeping data within 99-percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plot(\"Unscaled data\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large majority of the samples are compacted to a specific range, [0, 10] for the median income and [0, 6] for the average house occupancy. Note that there are some marginal outliers (some blocks have average occupancy of more than 1200)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Scaler\n",
    "- The motivation to use this scaling lies in the fact that many machine learning estimators might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance. In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "make_plot(\"Data after standard scaling\", X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.mean_, scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.mean(axis = 0), X_scaled.std(axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StandardScaler` removes the mean and scales the data to unit variance. The scaling shrinks the range of the feature values as shown in the left figure below.\n",
    "\n",
    "Due to outliers magnitude, however, most of the data lie in the [-2, 4] range for the transformed median income feature while the same data is squeezed in the smaller [-0.2, 0.2] range for the transformed average house occupancy. `StandardScaler` therefore cannot guarantee balanced feature scales in the presence of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MinMax Scaler\n",
    "\n",
    "- The motivation to use this scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "make_plot(\"Data after minmax scaling\", X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.min(axis = 0), X_scaled.max(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MinMaxScaler` rescales the data set such that all feature values are in\n",
    "the range [0, 1]. \n",
    "\n",
    "However, this scaling compresses all inliers into the narrow range [0, 0.005] for the transformed average house occupancy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `StandardScaler` and `MinMaxScaler` are very sensitive to the\n",
    "presence of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robust Scaler\n",
    "- Whenever data contains many outliers, scaling using the mean and variance of the data is likely to not work very well (same for MinMax scaling). In these cases, RobustScaler can be used as it exploits more robust estimates for the center and range of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler(quantile_range = (25.0, 75.0))\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "make_plot(\"Data after robust scaling\", X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.center_, scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RobustScaler` removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\n",
    "\n",
    "The centering and scaling statistics of `RobustScaler` are based on percentiles and are therefore not influenced by a small number of very large marginal outliers. \n",
    "Consequently, the resulting range of the transformed feature values is larger than for the previous scalers and, more importantly, are approximately similar: for both features most of the transformed values lie in a [-2, 3] range as seen in the zoomed-in figure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the outliers themselves are still present in the transformed data. If a separate outlier clipping is desirable, a non-linear transformation is required (e.g., by using `QuantileTransformer`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
