{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- [Classification: a first example](#Classification:-a-first-example)\n",
    "- [Model Evaluation](#Model-evaluation)\n",
    "    - Holdout evaluation\n",
    "    - *k*-fold Cross-Validation\n",
    "- [Evaluation Metrics for classification problems](#Evaluation-Metrics-for-classification-problems)\n",
    "- [Evaluation on Iris Dataset](#Evaluation-on-Iris-Dataset)\n",
    "- [Feature Selection](#Feature-Selection)\n",
    "- [Sampling and rebalancing with `imblearn`](#Sampling-and-rebalancing-with-imblearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: a first example\n",
    "\n",
    "K nearest neighbors (kNN) is one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class.\n",
    "\n",
    "![KNN](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/355px-KnnClassification.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on the IRIS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the documentation of the sklearn implementation of the KNN classifier.\n",
    "\n",
    "Typically, for a sklearn estimator, you can rely on the following sources:\n",
    "- API: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "- User guide: https://scikit-learn.org/stable/modules/neighbors.html#classification\n",
    "- Examples: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNeighborsClassifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Create the \"model\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Fit the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Use the model**: what kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal? call the \"predict\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = knn.predict([[3, 5, 4, 2],])\n",
    "print(result)\n",
    "print(iris.target_names[result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "The final goal of a classification algorithm is to correctly classify a previously unseen example. In other words we want to assess the **generalization capability** of our model. Therefore it is not sufficient to solve an optimization problem on the examples used for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout method\n",
    "In the holdout method, the input dataset is split into two separate sets:  **training set** and **test set**. \n",
    "\n",
    "- the **training set** is used during training in order to increase the experience of the model. An optimization procedure finds the parameters configuration which minimizes the training error.\n",
    "- the **test set** is used to measure the actual performance of the model, thus its generalization capability. \n",
    "\n",
    "The inference capability on previously unseen examples arises from an assumption about data generating process (i.i.d. assumption): examples in training and test sets are supposed to be independent from each other and identically distributed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is of the utmost importance that the actual test set is not used to make choices about the model and its parameters/hyperparameters.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *k*-fold cross-validation\n",
    "\n",
    "The **simple holdout evaluation approach can be problematic if the resulting test set is small**: the sampled test examples may not be representative of the actual distribution of our dataset. Training (and evaluating) our model on different random splits would results in different values of model performance. In other words, we may observe statistical uncertainty around the estimated average generalization error.\n",
    "\n",
    "To address this issue, *k*-fold cross validation is commonly adopted.\n",
    "The procedure consists in the following steps:\n",
    "- the dataset is split in *k* non-overlapping subset\n",
    "- for each subset *i* the model is evaluated on the *i-th* subset itself and trained on the union of the remaining *k-1* subsets,\n",
    "- after *k* iteration, we can rely on *k* values of model performance: the final score is obtained averaging the scores across the *k* trials.\n",
    "\n",
    "Using k-fold cross-validation allows to estimate the average generalization error using all the examples, at the price of an increased runtime (k training procedures)\n",
    "\n",
    "![kfoldcv](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both *single holdout validation* and *k-fold cross-validation* we can resort to the implementation provided in the `model_selection` module provided in the `scikit-learn` (sklearn) library.\n",
    "\n",
    "- `train_test_split`: [docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- `KFold` [docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn-model-selection-kfold), or (better) `StratifiedKFold` [docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html?highlight=stratified#sklearn.model_selection.StratifiedKFold), that is a variation of k-fold in which each set contains approximately the same percentage of samples of each target class as the complete set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for classification problems\n",
    "\n",
    "Machine learning models are often used to predict the outcomes of a classification problem. Predictive models rarely predict everything perfectly, so there are many performance metrics that can be used to analyze our models.\n",
    "\n",
    "When you run a prediction on your data to distinguish among two classes (*positive* and *negative* classes, for simplicity), your results can be broken down into 4 parts:\n",
    "\n",
    "<img src=\"images/classification_report.png\" alt=\"drawing\" width=\"450\"/>\n",
    "\n",
    "* **True Positives**: data in class *positive* that the model predicts will be in class *positive*;\n",
    "* **True Negatives**: data in class *negative* that the model predicts will be in class *negative*;\n",
    "* **False Positives**: data in class *negative* that the model predicts will be in class *positive*;\n",
    "* **False Negatives**: data in class *positive* that the model predicts will be in class *negative*.\n",
    "\n",
    "The most common performance metrics in this binary classification scenario are the following:\n",
    "\n",
    "* **accuracy**: the fraction of observations (both positive and negative) predicted correctly:\n",
    "\n",
    "$$ Accuracy = \\frac{(TP+TN)}{(TP+FP+TN+FN)} $$\n",
    "\n",
    "* **precision** (or **positive predictive value**): the fraction of predicted positive observations that are actually positive:\n",
    "\n",
    "$$ Precision = \\frac{TP}{(TP+FP)} $$\n",
    "\n",
    "* **recall** (or **sensitivity** or **True Positive Rate (TPR)**): the fraction of positive observations that are predicted correctly:\n",
    "\n",
    "$$ Recall = \\frac{TP}{(TP+FN)} $$\n",
    "\n",
    "* **False Positive Rate (FPR)** (or **1-specificity**): the fraction of negative observations that are wrongly predicted as positive (false alarm rate):\n",
    "\n",
    "$$ False Positive Rate = \\frac{FP}{(FP+TN)} $$\n",
    "\n",
    "\n",
    "* **f1-score**: a composite measure that combines both precision and recall (harmonic mean):\n",
    "\n",
    "$$ F_1 = \\frac{2 \\cdot P \\cdot R}{(P+R)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **confusion matrix** is useful for quickly calculating precision and recall given the predicted labels from a model. A confusion matrix for binary classification shows the four different outcomes: true positive, false positive, true negative, and false negative. The actual values form the columns, and the predicted values (labels) form the rows. The intersection of the rows and columns show one of the four outcomes. \n",
    "\n",
    "![confusion-matrix.png](images/confusion-matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have more than two classes?\n",
    "We can still plot the confusion matrix, as shown in the following example based on Iris dataset.\n",
    "![cm_multiclass](https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_001.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also still evaluate the **metrics per class**, in a OneClass-vs-Rest fashion. For instance:\n",
    "- $precision_{virginica}$: number of correctly predicted virginica records (9) out of all predicted verginica records (9+6+0=15), which amounts to 9/15=0.6\n",
    "- $recall_{virginica}$: number of correctly predicted virginica records (9) out of the number of actual viriginica records (9+0+0=9), which amounts to 9/9=1\n",
    "- $precision_{versicolor}$: number of correctly predicted versicolor records (10) out of all predicted versicolor records (10+0+0=10), which amounts to 10/10=1\n",
    "- ...\n",
    "\n",
    "and so on, for the other classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC (Receiver Operating Characteristic) curve is another useful tool to evaluate classifier output quality.\n",
    "ROC curves feature **true positive rate** on the Y axis, and **false positive rate** on the X axis.\n",
    "\n",
    "ROC curves are typically used in binary classification to study the output of a classifier. In order to extend ROC curve and ROC area to multi-label classification, it is necessary to binarize the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "X_b, y_b = make_classification(random_state = 2)\n",
    "\n",
    "plt.scatter(X_b[:, 0], \n",
    "            X_b[:, 1], \n",
    "            c = y_b, \n",
    "            edgecolors = \"k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b_train, X_b_test, y_b_train, y_b_test = train_test_split(X_b, y_b, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(random_state = 0, probability = True).fit(X_b_train, y_b_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b_pred = clf.predict_proba(X_b_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An actual curve (multiple points in the FPR-TPR space) can be obtained if we predict *probabilities*.\n",
    "\n",
    "If we predict *labels* directly, we simply obtain a single point in the FPR-TPR space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_b_test, y_b_pred[:, 1]) # display from predictions. There is also the \"from_estimator\" method.\n",
    "plt.plot([0, 1], [0, 1], color = \"navy\", lw = 2, linestyle = \"--\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset in training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3, \n",
    "                                                    random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the classification model based on the training set and predict the labels for the instances in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5).fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check: how is accuracy defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_test == y_pred) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, \n",
    "                                        y_pred,\n",
    "                                        display_labels = iris.target_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names = iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can get a dictionary from `classification_report`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, y_pred, target_names = iris.target_names, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on metrics averaging:\n",
    "- **macro**: Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "- **weighted**: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance.\n",
    "- **micro**: Calculate metrics globally by counting the total true positives, false negatives and false positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 5,\n",
    "           shuffle = True,\n",
    "           random_state = 123\n",
    "          )\n",
    "\n",
    "for enu,(train_index, test_index) in enumerate(kf.split(iris.data, iris.target)):\n",
    "    print(f'------------------------fold {enu}------------------------')\n",
    "    \n",
    "    print(\"TRAIN:\",train_index.shape)\n",
    "    print(train_index)\n",
    "    print(\"TEST:\", test_index.shape)\n",
    "    print(test_index)\n",
    "    print(\"TEST LABELS:\")\n",
    "    print(iris.target[test_index])\n",
    "    \n",
    "    # TODO: train model on the current training set\n",
    "    # TODO: test model on the current test set\n",
    "    \n",
    "    print(f'----------------------end fold {enu}----------------------','\\n')\n",
    "\n",
    "# compute average metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffling** may be essential to get a meaningful cross-validation result if data ordering is not arbitrary (i.e., same class labels are contiguous).\n",
    "By default no shuffling occurs, including for the (stratified) K fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we do not need to implement the loop above. Indeed, `sklearn` offers specific methods for evaluating models performance in cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(KNeighborsClassifier(), \n",
    "                X, \n",
    "                y, \n",
    "                cv = KFold(5, shuffle = True, random_state = 123)) \n",
    "# note that the default scorer for KNeighborsClassifier is the accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cross_validate` function differs from `cross_val_score` in two ways:\n",
    "- It allows specifying multiple metrics for evaluation.\n",
    "- It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "results = cross_validate(KNeighborsClassifier(), \n",
    "                         iris.data,\n",
    "                         iris.target,\n",
    "                         scoring = {\n",
    "                             'fscore': make_scorer(f1_score, average = 'macro'),\n",
    "                             'accuracy': make_scorer(accuracy_score)},\n",
    "                         return_estimator = True,\n",
    "                         cv = KFold(5, shuffle = True, random_state = 123),\n",
    "                         n_jobs = -1) # Number of jobs to run in parallel. \n",
    "                                      # Training the estimator and computing the score are parallelized over the cross-validation splits.\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to obtain the ***average* classification report** over cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 123)\n",
    "\n",
    "list_df = []\n",
    "list_accuracy = []\n",
    "\n",
    "k = 1\n",
    "for train, val in skf.split(iris.data, iris.target):\n",
    "    print(f'FOLD {k}')\n",
    "\n",
    "    # fit and predict using classifier\n",
    "    X_tr = iris.data[train]\n",
    "    y_tr = iris.target[train]\n",
    "    X_val = iris.data[val]\n",
    "    y_val = iris.target[val]\n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X_tr,y_tr)\n",
    "    y_pred = clf.predict(X_val)\n",
    "\n",
    "    # compute classification report\n",
    "    cr = classification_report(y_val, y_pred, output_dict = True, zero_division = np.nan) # important!\n",
    "    print(classification_report(y_val, y_pred, zero_division = np.nan))\n",
    "\n",
    "    # store accuracy\n",
    "    list_accuracy.append(cr['accuracy'])\n",
    "\n",
    "    # store per-class metrics as a dataframe\n",
    "    df = pd.DataFrame({k: v for k, v in cr.items() if k != 'accuracy'})\n",
    "    display(df)\n",
    "    list_df.append(df)\n",
    "    k+=1\n",
    "    \n",
    "\n",
    "# compute average per-class metrics    \n",
    "df_concat = pd.concat(list_df)\n",
    "grouped_by_row_index = df_concat.groupby(df_concat.index)\n",
    "df_avg = grouped_by_row_index.mean()\n",
    "\n",
    "# compute average accuracy\n",
    "accuracy_avg = np.mean(list_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate cross-validated estimates for each input data point, by juxtaposing the predictions obtained on the fold out at each iteration of the CV (remember that each sample belongs to exactly one test set).\n",
    "\n",
    "Passing these predictions into an evaluation metric may not be a valid way to measure generalization performance. \n",
    "Results can differ from cross_validate and cross_val_score unless all tests sets have equal size and the metric decomposes over samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_pred = cross_val_predict(clf, iris.data, iris.target, cv = skf)\n",
    "ConfusionMatrixDisplay.from_predictions(iris.target, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Feature selection/ dimensionality reduction can be used either to improve estimators' accuracy scores or to boost their performance on very high-dimensional datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A baseline: Variance threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes all features whose variance doesnâ€™t meet some threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_normalized = MinMaxScaler().fit_transform(X_train)\n",
    "X_normalized[:10, :] # display just the first ten rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized.var(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "fsel = VarianceThreshold(threshold = 0.05)\n",
    "fsel.fit_transform(X_normalized)[:10, :] # display just the first ten rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate feature selection\n",
    "Univariate feature selection works by examining each feature individually: the best features are selected based on univariate statistical tests. \n",
    "\n",
    "Scikit-learn exposes feature selection routines as objects that implement the *transform* method. The most popular *selection routines* are the following:\n",
    "- `SelectKBest`: Select features according to the *k* highest scores.\n",
    "- `SelectPercentile`: Select features according to a percentile of the highest scores (percent of features to keep).\n",
    "\n",
    "These selection routines take as input a *scoring function* that returns univariate scores:\n",
    "- classification\n",
    "    - `chi2`: Compute chi-squared stats between each non-negative feature and class.\n",
    "    - `f_classif`: Compute the ANOVA F-value for the provided sample. (*Note: F-test captures only linear dependency*)\n",
    "    - `mutual_info_classif`: Estimate mutual information for a discrete target variable. (*Note: mutual information can capture any kind of dependency between variables*)\n",
    "- regression\n",
    "    - `f_regression`: Univariate linear regression tests returning F-statistic and p-values.\n",
    "    - `mutual_info_regression`: Estimate mutual information for a continuous target variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A practical example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The iris dataset\n",
    "X, y = load_iris(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some noisy data not correlated\n",
    "E = np.random.RandomState(32).uniform(0, 0.1, size = (X.shape[0], 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the noisy data to the informative features\n",
    "X = np.hstack((X, E))\n",
    "display(pd.DataFrame(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to select feature and evaluate the classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X)[4].plot(kind = 'hist', bins = 7)\n",
    "plt.title('column 4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(f_classif, k = 4)\n",
    "selector.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_classif?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *f_classif* function evaluates the one-way ANOVA test.\n",
    "\n",
    "The one-way ANOVA tests the null hypothesis that 2 or more groups have    the same population mean. The test is applied to samples from two or     more groups, possibly with differing sizes. \n",
    "\n",
    "In our setting, the *groups* are the sets of samples belonging to a given class. Intuitively, if, for a given feature, the sets of samples belonging to different classes have the same mean, than that feature is \"**not useful**\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_statistics, p_values = f_classif(X[:, :2], y)\n",
    "print(f'f-stat Att.0: {f_statistics[0]:.4}  -  p-value Att.0: {p_values[0]:.3}')\n",
    "print(f'f-stat Att.1: {f_statistics[1]:.4}  -  p-value Att.1: {p_values[1]:.3}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first two attributes, the p-value is close to zero. With a level of significance $\\alpha$ (e.g. $\\alpha = 0.05$) we can reject the null hypothesis --> the three groups (one per class) have different mean --> the feature is \"**useful**\" and should be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices = np.arange(X.shape[-1])\n",
    "plt.bar(X_indices - 0.05, selector.pvalues_, width = 0.2)\n",
    "plt.title(\"Feature univariate score\")\n",
    "plt.xlabel(\"Feature number\")\n",
    "plt.ylabel(r\"Univariate score ($p_{value}$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = - np.log10(selector.pvalues_)\n",
    "scores /= scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_indices = np.arange(X.shape[-1])\n",
    "plt.bar(X_indices - 0.05, scores, width = 0.2)\n",
    "plt.title(\"Feature univariate score\")\n",
    "plt.xlabel(\"Feature number\")\n",
    "plt.ylabel(r\"Univariate score ($-Log(p_{value})$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_out = selector.transform(X_train)\n",
    "display(pd.DataFrame(X_train).head())\n",
    "display(pd.DataFrame(X_out).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive feature elimination (RFE)\n",
    "Suppose you can rely on an external estimator (classification/regression model) that assigns weights to features.\n",
    "\n",
    "In RFE, first the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute (e.g., `coef_`, `feature_importances_`)  or callable. Then, the least important feature (or features, if a step>1 is used) is pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(max_iter = 1000) \n",
    "\n",
    "# LogisticRegression has attribute \"coef_\", indicating coefficient of the features in the decision function.\n",
    "\n",
    "rfe = RFE(estimator = lr, n_features_to_select = 1, step = 1)\n",
    "rfe.fit(X_train, y_train)\n",
    "rfe.ranking_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection using `SelectFromModel`\n",
    "\n",
    "SelectFromModel is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as `coef_`, `feature_importances_`) or via an `importance_getter` callable after fitting. The features are considered unimportant and removed if the corresponding importance of the feature values are below the provided threshold parameter (or via other built-in heuristics).\n",
    "\n",
    "Differently from RFE, `SelectFromModel` involves no iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Feature Selection (SFS)\n",
    "\n",
    "It can be either forward or backward:\n",
    "- *Forward-SFS* is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero features and find the one feature that **maximizes a cross-validated score** when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the `n_features_to_select` parameter.\n",
    "- *Backward-SFS* follows the same idea but works in the opposite direction: instead of starting with no features and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used.\n",
    "\n",
    "\n",
    "As it relies on a cross-validated score for selecting the best feature at each iteration, SFS differs from RFE and SelectFromModel in that it does not require the underlying model to expose a `coef_` or `feature_importances_` attribute. It may however be slower considering that more models need to be evaluated, compared to the other approaches.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter = 1000)\n",
    "sfs = SequentialFeatureSelector(lr, \n",
    "                                cv = skf,\n",
    "                                scoring = 'accuracy', \n",
    "                                direction = 'forward', \n",
    "                                n_features_to_select = 4)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "sfs.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter = 1000)\n",
    "sfs = SequentialFeatureSelector(lr,\n",
    "                                cv = skf, \n",
    "                                scoring = 'accuracy',  \n",
    "                                direction = 'backward', \n",
    "                                n_features_to_select = 4)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "sfs.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 1**: Which one takes the longest and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit # what sort of magic is this? a brief aside at the end of the notebook\n",
    "\n",
    "lr = LogisticRegression(max_iter = 1000)\n",
    "sfs = SequentialFeatureSelector(lr, \n",
    "                                cv = skf,\n",
    "                                scoring = 'accuracy', \n",
    "                                direction ='forward', \n",
    "                                n_features_to_select = 4)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "sfs.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "lr = LogisticRegression(max_iter = 1000)\n",
    "sfs = SequentialFeatureSelector(lr, \n",
    "                                cv = skf, \n",
    "                                scoring = 'accuracy', \n",
    "                                direction = 'backward', \n",
    "                                n_features_to_select = 4)\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "sfs.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 2**: Why do we have noisy features among the selected ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check manually the first step, e.g. in the forward fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[1]):\n",
    "    print(np.mean(cross_val_score(LogisticRegression(max_iter = 1000), \n",
    "                                  X_train[:, i].reshape(-1, 1),\n",
    "                                  y_train, \n",
    "                                  cv = skf)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest score (around 0.96) is obtained for the fourth attribute (index = 3).\n",
    "\n",
    "Evaluate the second step: which feature should be added? (i.e., which one improves most the cross-val-score?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[1]):\n",
    "    if i == 3:\n",
    "        continue\n",
    "    X_new = np.concatenate((X_train[:, i].reshape(-1, 1), (X_train[:, 3].reshape(-1, 1))),\n",
    "                           axis = 1)\n",
    "    print(i, np.mean(cross_val_score(LogisticRegression(max_iter = 1000), \n",
    "                                    X_new,\n",
    "                                    y_train, \n",
    "                                    cv = skf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that, in our setting, the other actual features from the Iris dataset (X0, X1, X2) worsen the prediction, rather than improving it. That's why, if we set the target number of features to be selected to 4, the algorithm favors the noisy ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting an even minimum tolerance value (features are selected until the score improvement does not exceed such a threshold), the behavior is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter = 1000)\n",
    "sfs = SequentialFeatureSelector(lr, \n",
    "                                cv = skf, \n",
    "                                tol = 0.0000001, \n",
    "                                scoring = 'accuracy',  \n",
    "                                direction ='forward', \n",
    "                                n_features_to_select = 'auto')\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "sfs.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and rebalancing with `imblearn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Imbalanced-learn (imported as `imblearn`) is a library relying on scikit-learn and provides tools when dealing with classification with imbalanced classes. \n",
    "As such, it provides several samplers, which follows the scikit-learn API using the base estimator and implements a sampling functionality through the `fit_resample` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an imbalanced dataset using sklearn utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.title(\"Synthetic normally distributed dataset\")\n",
    "X1, Y1 = make_blobs(n_samples = [1000, 100, 500], n_features = 2,random_state = 1)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker = \"o\", c = Y1, s = 25, edgecolor = \"k\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(Y1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "sampler = RandomUnderSampler(random_state = 42)\n",
    "X1_RUS, Y1_RUS = sampler.fit_resample(X1, Y1)\n",
    "print(pd.Series(Y1_RUS).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "sampler = RandomOverSampler(random_state = 42)\n",
    "X1_ROS, Y1_ROS = sampler.fit_resample(X1, Y1)\n",
    "print(pd.Series(Y1_ROS).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sampler = SMOTE(random_state = 42)\n",
    "X1_SMOTE, Y1_SMOTE = sampler.fit_resample(X1, Y1)\n",
    "print(pd.Series(Y1_SMOTE).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All rebalancing methods ensure flexibility about sampling strategy and target ratio between classes: check the docs and specifically the `sampling_strategy` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual comparison of the three methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 4, figsize = (20, 5))\n",
    "\n",
    "ax[0].scatter(X1[:, 0], X1[:, 1], marker = \"o\", c = Y1, s = 25, edgecolor = \"k\")\n",
    "ax[0].set_title('original')\n",
    "\n",
    "ax[1].scatter(X1_RUS[:, 0], X1_RUS[:, 1], marker = \"o\", c = Y1_RUS, s = 25, edgecolor = \"k\")\n",
    "ax[1].set_title('Random UnderSampling')\n",
    "\n",
    "ax[2].scatter(X1_ROS[:, 0], X1_ROS[:, 1], marker = \"o\", c = Y1_ROS, s = 25, edgecolor = \"k\")\n",
    "ax[2].set_title('Random OverSampling')\n",
    "\n",
    "ax[3].scatter(X1_SMOTE[:, 0], X1_SMOTE[:, 1], marker = \"o\", c = Y1_SMOTE, s = 25, edgecolor = \"k\")\n",
    "ax[3].set_title('SMOTE')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several other methods/extensions are available in imblearn (see the [docs](https://imbalanced-learn.org/stable/references/index.html#api))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: *Magic* Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[IPython's 'magic' functions](https://ipython.readthedocs.io/en/stable/interactive/magics.html)\n",
    "\n",
    "- The magic function system provides a series of **functions which allow you to\n",
    "control the behavior of IPython itself**, plus a lot of system-type\n",
    "features. There are two kinds of magics, **line-oriented** and **cell-oriented**:\n",
    "    - **Line magics are prefixed with the % character** and work much like OS\n",
    "command-line calls. They get as an argument the rest of the line, where\n",
    "arguments are passed without parentheses or quotes.\n",
    "    - **Cell magics are prefixed with %% (a double % character)**, and they are functions that get as an argument not only the rest of the line, but also the lines below it in a separate argument. These magics are called with two arguments: the rest of the call line and the body of the cell, consisting of the lines below the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit enu * i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
