{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "\n",
    "- **[Series](#Series)**\n",
    "- **[DataFrame](#DataFrame)**\n",
    "    - [Accessing a DataFrame](#Accessing-a-DataFrame)\n",
    "    - [Boolean Indexing](#Boolean-Indexing)\n",
    "    - [Adding columns and rows](#Adding-columns-and-rows)\n",
    "    - [Deleting columns and rows](#Deleting-columns-and-rows)\n",
    "    - [Reading and Writing DataFrames](#Reading-and-Writing-DataFrames)\n",
    "    - [Missing Data](#Missing-data)\n",
    "- **[DataFrame Operations](#DataFrame-Operations)**\n",
    "    - [Matrix operations](#Matrix-operations)\n",
    "    - [Column operations](#Column-operations)\n",
    "    - [Data Splitting](#Data-Splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data Mining Process \n",
    "\n",
    "Image from: \n",
    "\n",
    "    Usama M. Fayyad, Gregory Piatetsky-Shapiro, Padhraic Smyth, and Ramasamy Uthurusamy. \n",
    "    Advances in Knowledge Discovery and Data Mining. MIT Press, Menlo Park, CA, 1996\n",
    "\n",
    "![datamining](images/DataMiningProcess.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas** is desgined to make **data pre-processing and data analysis fast and easy in Python**. Pandas adopts many coding idioms from NumPy, such as avoiding the `for` loops, but it is designed for working with heterogenous data represented in tabular format.\n",
    "\n",
    "To use Pandas, you need to import the `pandas` module, using for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T12:57:09.909716Z",
     "start_time": "2020-06-24T12:57:02.830486Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # we will also need numpy\n",
    "\n",
    "print('numpy', np.__version__)\n",
    "print('pandas', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This import style is quite standard; all objects and functions the `pandas` package will now be invoked with the `pd.` prefix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Numpy\n",
    "NumPy (**Num**erical **Py**thon) is the fundamental package for scientific computing with Python. It contains, among other things:\n",
    "\n",
    "- a powerful N-dimensional array object\n",
    "- sophisticated functions that support broadcasting (i.e., it allows to perform arithmetic operations between arrays with different shape)\n",
    "- tools for integrating C/C++ and Fortran code\n",
    "- useful linear algebra, Fourier transform, and random number capabilities\n",
    "\n",
    "The core object of numpy is **ndarray**: N-dimensional Array. It represents a *multidimensional*, *homogeneous* array of *fixed-size items*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of 1-dimensional array\n",
    "np.arange(0, 1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of 2-dimensional array\n",
    "my_ndarray = np.zeros((3, 5))\n",
    "my_ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.asarray([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_ndarray)\n",
    "print(my_ndarray.shape)\n",
    "print(my_ndarray.ndim)\n",
    "print(my_ndarray.size)\n",
    "print(my_ndarray.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several NumPy functions for [creating arrays](https://docs.scipy.org/doc/numpy/user/quickstart.html#array-creation):\n",
    "\n",
    "| Function | Description |\n",
    "| ---: | :--- |\n",
    "| `np.array(a)` | Create $n$-dimensional NumPy array from sequence `a` |\n",
    "| `np.linspace(a, b, N)` | Create 1D NumPy array with `N` equally spaced values from `a` to `b` (inclusively)|\n",
    "| `np.arange(a, b, step)` | Create 1D NumPy array with values from `a` to `b` (exclusively) incremented by `step`|\n",
    "| `np.zeros(N)` | Create 1D NumPy array of zeros of length $N$ |\n",
    "| `np.zeros((n, m))` | Create 2D NumPy array of zeros with $n$ rows and $m$ columns |\n",
    "| `np.ones(N)` | Create 1D NumPy array of ones of length $N$ |\n",
    "| `np.ones((n, m))` | Create 2D NumPy array of ones with $n$ rows and $m$ columns |\n",
    "| `np.eye(N)` | Create 2D NumPy array with $N$ rows and $N$ columns with ones on the diagonal (ie. the identity matrix of size $N$) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Functions\n",
    "\n",
    "[Mathematical functions](http://docs.scipy.org/doc/numpy/reference/routines.math.html) in NumPy are called [**universal functions**](https://docs.scipy.org/doc/numpy/user/quickstart.html#universal-functions) (ufuncs) and are *vectorized*. Vectorized functions operate *element-wise* on arrays producing arrays as output and are built to compute values across arrays *very* quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table contains a list of the most important **unary** ufuncs.\n",
    "\n",
    "|Function| Description |\n",
    "|:-------|:---------|\n",
    "|`np.abs`|Compute the absolute value element-wise for integer, floating-point, or complex values|\n",
    "|`np.sqrt`|Compute the square root of each element|\n",
    "|`np.exp`|Compute the exponent $e^x$ of each element|\n",
    "|`np.log`, `np.log10`, `np.log2`, `np.log1p`|Natural logarithm (base e), log base 10, log base 2, and log(1 + x), respectively|\n",
    "|`np.sign`|Compute the sign of each element: 1 (positive), 0 (zero), or –1 (negative)|\n",
    "|`np.ceil`|Compute the ceiling of each element|\n",
    "|`np.floor`|Compute the floor of each element|\n",
    "|`np.modf`|Return fractional and integral parts of array as a separate array|\n",
    "|`np.isnan`|Return boolean array indicating whether each value is `NaN` (Not a Number)|\n",
    "|`np.cos`, `np.cosh`, `np.sin`, `np.sinh`, `np.tan`, `np.tanh`|Regular and hyperbolic trigonometric functions|\n",
    "|`np.arccos`, `np.arccosh`, `np.arcsin`, `np.arcsinh`, `np.arctan`, `np.arctanh`|Inverse trigonometric functions|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table contains a list of the most important **binary** ufuncs.\n",
    "\n",
    "|Function| Description |\n",
    "|:-------|:---------|\n",
    "|`np.add`|Element-wise addition|\n",
    "|`np.subtract`|Element-wise subtraction|\n",
    "|`np.multiply`|Element-wise multiplication|\n",
    "|`np.divide`|Element-wise division|\n",
    "|`np.mod`|Element-wise modulus|\n",
    "|`np.power`|Raise elements in first array to powers indicated in second array |\n",
    "|`np.maximum`, `np.fmax`|Element-wise maximum; `np.fmax` ignores `NaN`|\n",
    "|`np.minimum`, `np.fmin`|Element-wise minimum; `np.fmin` ignores `NaN`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Pandas has two main data structures, **Series** and **DataFrame**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Series\n",
    "\n",
    "Series are the Pandas version of 1-D Numpy arrays. \n",
    "\n",
    "An instance of Series is a single dimension array-like object containing:\n",
    "- a *sequence of values*,\n",
    "- an array of *data labels*, namely its **index**.\n",
    "\n",
    "A Series can be created easily from a Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:06:29.883282Z",
     "start_time": "2020-06-24T14:06:29.873731Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = pd.Series([4, 8, 1, 3])\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string representation of a Series display two columns: the first column represents the index array, the second column represents the values array. Since no index was specified, the default indexing consists of increasing integers starting from 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying structure can be recovered with the `values` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:06:30.975916Z",
     "start_time": "2020-06-24T14:06:30.966676Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ts.values)\n",
    "print(type(ts.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a Series with its own index, you can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T13:10:32.009532Z",
     "start_time": "2020-06-24T13:10:31.999642Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = pd.Series([4, 8, 1, 3], index = ['first', 'second', 'third', 'fourth'])\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels in the index can be used to select values in the Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T13:13:05.940761Z",
     "start_time": "2020-06-24T13:13:05.922266Z"
    }
   },
   "outputs": [],
   "source": [
    "print(ts['first'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ts[['second', 'fourth']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think about a Series as a kind of fixed-length, ordered Python's `dict`, mapping index values to data values. In fact, it is possible to create a Series directlty from a Python's `dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T13:18:31.486140Z",
     "start_time": "2020-06-24T13:18:31.465527Z"
    }
   },
   "outputs": [],
   "source": [
    "my_dict = {'Pisa': 80, 'London': 300, 'Paris': 1}\n",
    "ts = pd.Series(my_dict)\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting a series:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sort by values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sort by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts.sort_values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **no pandas method has the side effect of modifying your data; almost every method returns a new object, leaving the original object untouched. If the data is modified, it is because you did so explicitly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "\n",
    "A DataFrame is a **rectangular table of data**. It contains an ordered list of columns. Every column can be of a different type. \n",
    "\n",
    "A DataFrame has both a *row index* and a *column index*. It can be thought as a *dictionary of Series* (one per column) all sharing the same index labels.\n",
    "\n",
    "There are many ways to construct a DataFrame: most common ways are using a dictionary of Python's lists, dictionary of NumPy's arrays, dictionary of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T13:46:58.794613Z",
     "start_time": "2020-06-24T13:46:58.779817Z"
    }
   },
   "outputs": [],
   "source": [
    "cars = {'Brand': ['Honda Civic', 'Toyota Corolla', 'Ford Focus', 'Audi A4'],\n",
    "        'Price': [22000, 25000, 27000, 35000],\n",
    "        'Wheels': 4} # broadcast if possible\n",
    "\n",
    "df = pd.DataFrame(cars)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting DataFrame will receive its index automatically as with Series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pretty-print a DataFrame in a Jupyter notebooks, it is enough to write its name (or using the `head()` instance method for very long DataFrames):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the T attribute, to transpose a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T13:48:19.586597Z",
     "start_time": "2020-06-24T13:48:19.564159Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the *numerical* data is provided by `describe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # by default it includes only \"numbers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain generic information about the dataset, including columns name and non null count with \n",
    "`info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the column names with `keys()` method or `column` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:19:07.147448Z",
     "start_time": "2020-06-24T14:19:07.137275Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many feature from the NumPy package can be directly used with Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:48:02.257248Z",
     "start_time": "2020-06-25T08:48:02.236802Z"
    }
   },
   "outputs": [],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:48:03.529820Z",
     "start_time": "2020-06-25T08:48:03.499697Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing a DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a brand new DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:10:32.187393Z",
     "start_time": "2020-06-24T14:10:32.162017Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_of_list = {'birth': [1860, 1770, 1858, 1906], \n",
    "                'death': [1911, 1827, 1924, 1975], \n",
    "                'city': ['Kaliste', 'Bonn', 'Lucques', 'Saint-Petersburg']}\n",
    "composers_df = pd.DataFrame(dict_of_list, index = ['Mahler', 'Beethoven', 'Puccini', 'Shostakovich'])\n",
    "composers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways of accessing values or series of values in a Dataframe. Unlike in Series, a simple bracket gives access to a column and not an index, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:11:08.259631Z",
     "start_time": "2020-06-24T14:11:08.241496Z"
    }
   },
   "outputs": [],
   "source": [
    "composers_df['city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns a Series. Alternatively one can also use the attributes syntax and access columns by using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:11:37.321286Z",
     "start_time": "2020-06-24T14:11:37.309806Z"
    }
   },
   "outputs": [],
   "source": [
    "composers_df.city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attributes syntax has some limitations, so in case something does not work as expected, revert to the brackets notation.\n",
    "\n",
    "When specifiying multiple columns, a DataFrame is returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:12:06.558313Z",
     "start_time": "2020-06-24T14:12:06.540600Z"
    }
   },
   "outputs": [],
   "source": [
    "composers_df[['city', 'birth']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard indexing operators (just slices the rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the [docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html):\n",
    ">The Python and NumPy indexing operators [$\\cdot$] and attribute operator  `.` provide quick and easy access to pandas data structures across a wide range of use cases. This makes interactive work intuitive, as there’s little new to learn if you already know how to deal with Python dictionaries and NumPy arrays. However, since the type of the data to be accessed isn’t known in advance, directly using standard operators has some optimization limits. For production code, we recommended that you take advantage of the **optimized pandas data access methods**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas optimized data access methods:  `iloc` and `loc`.\n",
    "\n",
    "**Remember that `loc` and `iloc` are attributes, not methods, hence they use brackets `[]` and not parenthesis `()`.**\n",
    "\n",
    "The `loc` attribute allows to recover elements by using the index labels, while the `iloc` attribute can be used to recover the regular indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:16:36.990555Z",
     "start_time": "2020-06-24T14:16:36.979449Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "composers_df.iloc[0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df.loc[['Mahler', 'Beethoven'], 'death']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df.loc['Beethoven', 'death']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Indexing\n",
    "\n",
    "Just like with Numpy, it is possible to subselect parts of a Dataframe using boolean indexing.\n",
    "A logical Series can be used as an index to select elements in the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = composers_df['death'] > 1859\n",
    "print(mask)\n",
    "composers_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More compact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df[composers_df['birth'] > 1900]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up: basics of indexing\n",
    "| Operation | Syntax | Result |\n",
    "| :---: | :---: | :---: |\n",
    "| Select column | `df[col]` (or `df.col`, where possible) | Series |\n",
    "| Select row by label | `df.loc[label]` | Series |\n",
    "| Select row by integer location | `df.iloc[loc]` | Series |\n",
    "| Slice rows | `df[5:10]` | DataFrame |\n",
    "| Select rows by boolean vector | `df[bool_vect]` | DataFrame |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding columns and rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very simple to add a column to a Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:25:29.267510Z",
     "start_time": "2020-06-24T14:25:29.253143Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "composers_df['country'] = '???' # broadcast if possible\n",
    "composers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, an existing list can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:25:30.513553Z",
     "start_time": "2020-06-24T14:25:30.500562Z"
    }
   },
   "outputs": [],
   "source": [
    "composers_df['country2'] = ['Austria', 'Germany', 'Italy', 'Russia']\n",
    "composers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame or a Series can be \"appended\" to another DataFrame through `pd.concat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = pd.DataFrame({'Sibelius': {'birth': None, 'death': 1900, 'city': None, 'country': None}}).T\n",
    "new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((composers_df, new_row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on [Database-style DataFrame or named Series joining/merging](https://pandas.pydata.org/docs/user_guide/merging.html#database-style-dataframe-or-named-series-joining-merging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting columns and rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df.drop(columns = ['country2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = composers_df.drop('Puccini')\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df #note that, by default, drop does not operate in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#composers_df.drop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common way of \"creating\" a Pandas Dataframe is by importing a table from another format like CSV (comma separated values) or Excel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('out/foo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = pd.read_csv('out/foo.csv')\n",
    "df_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_read = pd.read_csv('out/foo.csv', index_col = 0)\n",
    "df_read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Excel table is provided in the [composers.xlsx](data/composers.xlsx) file and can be read with the `pd.read_excel` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to install `openpyxl` package through `pip install openpyxl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:46:22.696917Z",
     "start_time": "2020-06-24T14:46:22.651062Z"
    }
   },
   "outputs": [],
   "source": [
    "composers_df = pd.read_excel('dataset/composers.xlsx')\n",
    "composers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader automatically recognized the heaers of the file. However it created a new index. If needed we can specify which column to use as header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_excel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:47:02.153798Z",
     "start_time": "2020-06-24T14:47:02.086758Z"
    }
   },
   "outputs": [],
   "source": [
    "composers_df = pd.read_excel('dataset/composers.xlsx', index_col = 'composer')\n",
    "composers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we open the file in Excel, we see that it is composed of more than one sheet. Clearly, when not specifying anything, the reader only reads the first sheet. However we can specify a sheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:51:26.183305Z",
     "start_time": "2020-06-24T14:51:26.141838Z"
    }
   },
   "outputs": [],
   "source": [
    "composers_df = pd.read_excel('dataset/composers.xlsx', index_col = 'composer', sheet_name = 'Sheet2')\n",
    "composers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, some information is missing. Some missing values are marked as \"`unknown`\" while other are `NaN`. `NaN` is the standard symbol for unknown/missing values and is understood by Pandas while \"`unknown`\" is just seen as text. \n",
    "This is impractical as now we have columns with a mix of numbers and text which will make later computations difficult. What we would like to do is to replace all \"irrelevant\" values with the standard `NaN` symbol that says \"*no information*\".\n",
    "For this we can use the `na_values` argument to specify what should be a `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T14:58:59.168497Z",
     "start_time": "2020-06-24T14:58:59.139749Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "composers_df = pd.read_excel('dataset/composers.xlsx', index_col = 'composer', sheet_name = 'Sheet2', na_values = ['unknown'])\n",
    "composers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read / Write SQL database\n",
    "\n",
    "from the [docs](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from SQL via either a SQL query or a SQL tablename. \n",
    "Note that when using a SQLite database only SQL queries are accepted, providing only the SQL tablename will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlite3 import connect\n",
    "# most common way to force an SQLite database to exist purely in memory \n",
    "conn = connect(':memory:') \n",
    "df = pd.DataFrame(data=[[0, '10/11/12'], [1, '12/11/10']], columns = ['int_column', 'date_column'])\n",
    "df.to_sql('test_data', conn) \n",
    "# Returns number of rows affected by to_sql, or None if the callable passed into method does not return an integer number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_sql('SELECT int_column, date_column FROM test_data', conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "pandas primarily uses the value `np.nan` to represent missing data. It is by default not included in computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = composers_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pd.isnull` (or `pd.isna`) and `pd.notnull` (or `pd.notna`) functions detects missing data. There are also corresponding **instance methods**. Get a boolean mask where values are `np.nan` (same for Series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.isna(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.isna().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Operations\n",
    "\n",
    "One of the great advantages of using Pandas to handle tabular data is how simple it is to extract valuable information from them. Here we are going to see various types of operations that are available for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Matrix operations\n",
    "\n",
    "The strength of Numpy is its natural way of handling matrix operations, and Pandas reuses a lot of these features. For example one can use simple mathematical operations to opereate at the cell level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T15:09:22.426144Z",
     "start_time": "2020-06-24T15:09:22.385407Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('dataset/composers.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T15:09:39.792603Z",
     "start_time": "2020-06-24T15:09:39.775993Z"
    }
   },
   "outputs": [],
   "source": [
    "2023-df['birth'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T15:10:00.912276Z",
     "start_time": "2020-06-24T15:10:00.899426Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.log(df['birth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly use an operation's output to create a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T15:19:14.832808Z",
     "start_time": "2020-06-24T15:19:14.807838Z"
    }
   },
   "outputs": [],
   "source": [
    "df['age'] = df['death'] - df['birth']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column operations\n",
    "\n",
    "There are other types of functions whose purpose is to summarize the data, e.g., by computing the mean or standard deviation. Pandas by default applies such functions column-wise and return a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T15:11:31.318490Z",
     "start_time": "2020-06-24T15:11:31.299730Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['birth', 'death', 'age']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes one needs to apply to a column a very specific function that is not provided by default. In that case we can use one of the different `apply` methods of Pandas.\n",
    "\n",
    "The simplest case is to apply a function to a column, or Series of a DataFrame. Let's say for example that we want to define the age >60 as 'old' and <60 as 'young'. We can define the following general function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T15:16:11.585908Z",
     "start_time": "2020-06-24T15:16:11.579767Z"
    }
   },
   "outputs": [],
   "source": [
    "def define_age(x):\n",
    "    if x>60:\n",
    "        return 'old'\n",
    "    else:\n",
    "        return 'young'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply this function on an entire Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T15:19:19.576840Z",
     "start_time": "2020-06-24T15:19:19.558825Z"
    }
   },
   "outputs": [],
   "source": [
    "df['categorical age'] = df.age.apply(define_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['compact categorical age'] = df.age.apply(lambda x: 'old' if x > 60 else 'young') # as before, but more compact\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categorical age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['categorical age'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Often Pandas tables mix regular variables (e.g. the size of cells in microscopy images) with categorical variables (e.g. the type of cell to which they belong). In that case, it is quite usual to split the data using the category to do computations. Pandas allows to do this very easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df = pd.read_excel('dataset/composers.xlsx', index_col = 'composer', sheet_name = 'Sheet5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:56:13.601372Z",
     "start_time": "2020-06-25T08:56:13.518195Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "composers_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want now to count how many composers we have in each category? \n",
    "\n",
    "Pandas simplifies this with the `groupby()` function, which actually groups elements by a certain criteria, e.g. a categorical variable like the period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:57:05.723610Z",
     "start_time": "2020-06-25T08:57:05.693209Z"
    }
   },
   "outputs": [],
   "source": [
    "composer_grouped = composers_df.groupby('period')\n",
    "composer_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a bit cryptic. What we actually have is a new object called *group* which has a lot of handy properties. First let's see what the groups actually are. As for the Dataframe, let's look at a summary of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:57:44.681494Z",
     "start_time": "2020-06-25T08:57:44.486278Z"
    }
   },
   "outputs": [],
   "source": [
    "composer_grouped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composer_grouped.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a dataframe with a statistical summary of the the contents. The \"names\" of the groups are here the indices of the Dataframe. These names are simply all the different categories that were present in the column we used for grouping. Now we can recover a single group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:58:36.933456Z",
     "start_time": "2020-06-25T08:58:36.907796Z"
    }
   },
   "outputs": [],
   "source": [
    "composer_grouped.get_group('baroque')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one has multiple categorical variables, one can also do a grouping on several levels. For example here we want to classify composers both by period and country. For this we just give two column names to the `groupby()` function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T08:59:41.133661Z",
     "start_time": "2020-06-25T08:59:41.051149Z"
    }
   },
   "outputs": [],
   "source": [
    "composer_grouped = composers_df.groupby(['period', 'country'])\n",
    "composer_grouped.get_group(('baroque', 'Germany'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in composer_grouped:\n",
    "    print(k)\n",
    "    display(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of this Group object is that it allows us to do very quickly both computations and plotting without having to loop through different categories. Indeed Pandas makes all the work for us: it applies functions on each group and then reassembles the results into a Dataframe (or Series depending on output).\n",
    "For example we can apply most functions we used for Dataframes (mean, sum etc.) on groups as well and Pandas seamlessly does the work for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` objects have also methods for convenient plotting. More on this in the next lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composers_df.birth.plot(kind = 'hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'a': [1, 5], 'b': [8, 0]})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2])\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_row_index = df_concat.groupby(df_concat.index)\n",
    "for index_value, group in by_row_index:\n",
    "    display(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_avg = by_row_index.mean()\n",
    "df_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: SettingWithCopyWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following example dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"A\": [1, 2, 3, 4, 5],\n",
    "    \"B\": [3.125, 4.12, 3.1, 6.2, 7.]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between these two operations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Chained indexing\n",
    "df[df['A'] > 2]['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: using .loc\n",
    "df.loc[df['A'] > 2, 'B'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These both yield the same results. However, method 2 (`.loc`) is much preferred over method 1 (chained indexing).\n",
    "- In method 1 `df[df['A'] > 2]` performs a first operation and then another operation selects the series indexed by `'B'`: pandas sees these operations as separate events.\n",
    "- In method 2 the `.loc` operation allows pandas to deal with this as a single entity, and is typically faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things become more complicated if such indexing is used for an assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['A'] > 2]['B'] = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on under the hood:\n",
    "```python\n",
    "df[df['A'] > 2]['B'] = 99\n",
    "```\n",
    "is basically equivalent to:\n",
    "```python\n",
    "df.__getitem__(df['A'] > 2).__setitem__('B', 99)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [docs](https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-view-versus-copy):\n",
    "\n",
    ">*It’s very hard to predict whether a `__getitem__` will return a view or a copy (it depends on the memory layout of the array, about which pandas makes no guarantees), and therefore whether the* `__setitem__` *will modify `df` or a temporary object that gets thrown out immediately afterward. **That’s what*** `SettingWithCopy`  ***is warning you about!***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['A'] > 2, 'B'] = 99\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, `df.loc` is guaranteed to be `df` itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takeaways:\n",
    "- avoid chained indexing and use `loc` or `iloc` instead;\n",
    "- use `DataFrame.copy(deep=True)` for obtaining an explicit copy and avoid unintended modifications to the original DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Towards Pandas 3.0: Copy-on-Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html#previous-behavior):\n",
    "> *pandas indexing behavior is tricky to understand. Some operations return views while other return copies. Depending on the result of the operation, mutating one object might accidentally mutate another*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pandas 1.5, a new mechanism named Copy-on-Write has been introduced to avoid unintended behaviour. [Copy-on-Write](https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html#copy-on-write-cow)  will become the default in pandas 3.0. It is recommended to turn it on now to benefit from all improvements.\n",
    "\n",
    "- CoW will lead to more predictable behavior since it is not possible to update more than one object with one statement, e.g. indexing operations or methods won’t have side-effects.\n",
    "- CoW means that any DataFrame or Series derived from another in any way always behaves as a copy. As a consequence, we can only change the values of an object through modifying the object itself. CoW disallows updating a DataFrame or a Series that shares data with another DataFrame or Series object inplace.\n",
    "- This avoids side-effects when modifying values and hence, most methods can avoid actually copying the data and only trigger a copy when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = False # default behaviour for pandas < 3.0\n",
    "\n",
    "df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]})\n",
    "\n",
    "print('\\n','this is DF')\n",
    "display(df)\n",
    "\n",
    "subset = df[\"foo\"]\n",
    "\n",
    "print('\\n','this is SUBSET')\n",
    "display(subset)\n",
    "\n",
    "subset.iloc[0] = 100\n",
    "print('\\n','this is MODIFIED SUBSET')\n",
    "display(subset)\n",
    "\n",
    "print('\\n','this is DF')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutating `subset`, e.g. updating its values, also updates `df`. The exact behavior is hard to predict. Copy-on-Write solves accidentally modifying more than one object, it explicitly disallows this. With CoW enabled, `df` is unchanged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True # default behaviour for pandas >= 3.0\n",
    "\n",
    "df = pd.DataFrame({\"foo\": [1, 2, 3], \"bar\": [4, 5, 6]})\n",
    "\n",
    "print('\\n','this is DF')\n",
    "display(df)\n",
    "\n",
    "subset = df[\"foo\"]\n",
    "\n",
    "print('\\n','this is SUBSET')\n",
    "display(subset)\n",
    "\n",
    "subset.iloc[0] = 100\n",
    "print('\\n','this is MODIFIED SUBSET')\n",
    "display(subset)\n",
    "\n",
    "print('\\n','this is DF')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chained assignment will never work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = False # default behaviour for pandas < 3.0\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"A\": [1, 2, 3, 4, 5],\n",
    "    \"B\": [3.125, 4.12, 3.1, 6.2, 7.]\n",
    "})\n",
    "display(df)\n",
    "df[df['A'] > 2]['B']=99\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.copy_on_write = True # default behaviour for pandas >= 3.0\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"A\": [1, 2, 3, 4, 5],\n",
    "    \"B\": [3.125, 4.12, 3.1, 6.2, 7.]\n",
    "})\n",
    "display(df)\n",
    "df[df['A'] > 2]['B']=99\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
